- data=aime_val
- model=gemini_2_5_flash_lite_thinking
- generation=gemini_thinking_16k
- prompt.problem_data=experiments/aime_val/selection_mistake/prompt_info.json
- generation.ignore_cache=true
- generation.max_tokens=4096
- generation.n=3
