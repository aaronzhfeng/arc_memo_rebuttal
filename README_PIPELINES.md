# AIME Memory-Augmented Pipelines

Two approaches to learn and retrieve lessons for AIME problem solving.

---

## ğŸ“ Pipeline Structure

### **Self-Reflective Learning**
- **Files**: `RUN_AIME_SELF_REFLECTIVE.md` (full), `RUN_AIME_SELF_REFLECTIVE_DEBUG.md` (debug)
- **Directory**: `experiments_self_reflective/`
- **Approach**: Model learns from its own reasoning without ground truth labels
- **Philosophy**: Meta-cognitive self-improvement through reflection

### **Label-Guided Learning**
- **Files**: `RUN_AIME_LABEL_GUIDED.md` (full), `RUN_AIME_LABEL_GUIDED_DEBUG.md` (debug)
- **Directory**: `experiments_label_guided/`
- **Approach**: Model learns only from verified correct solutions
- **Philosophy**: Quality through correctness filtering

---

## ğŸ”„ Pipeline Comparison

| Aspect | Self-Reflective | Label-Guided |
|--------|----------------|--------------|
| **Training Data** | All problems | Only correct problems |
| **Label Dependency** | âŒ No labels needed | âœ… Requires ground truth |
| **Lesson Quality** | Mixed (includes mistakes) | High (verified correct) |
| **Lesson Coverage** | 100% of training set | ~70-80% (pass@2 rate) |
| **Scalability** | âœ… Works without labels | âš ï¸ Needs verification |
| **Use Case** | Exploration, no labels | Maximizing accuracy |

---

## ğŸ¯ Key Features (Both Pipelines)

### 1. **Top 2 Lessons**
- `prompt.hint_lessons_limit=2`
- Reduces noise, keeps most relevant guidance
- Down from previous 5 lessons

### 2. **Self-Assessment Prompt**
- *"If you consider this problem straightforward to solve, you may disregard the following lessons..."*
- Gives model agency to ignore unhelpful lessons
- Prevents memory from hurting on easy problems

### 3. **Automated Analysis Reports**
- âœ… `evaluation_results.json` - Accuracy, errors, comparison
- âœ… Cost analysis - Token usage and API costs per stage
- âœ… Reflective usage - Which lessons helped (label-guided)
- âœ… Generated automatically at the end of each run

### 4. **Organized Output Structure**
```
experiments_{self_reflective|label_guided}/
â”œâ”€â”€ aime_train/          # Training stage outputs
â”‚   â”œâ”€â”€ joint_*/         # OR solve_*/
â”‚   â”‚   â””â”€â”€ evaluation_results.json  # Training accuracy
â”‚   â”œâ”€â”€ self_reflect_*/  # OR reasoning_*/
â”‚   â””â”€â”€ abstraction_*/
â”‚       â”œâ”€â”€ lessons.json              # All lessons
â”‚       â””â”€â”€ lessons_correct.json      # Filtered (label-guided)
â”œâ”€â”€ aime_val/            # Validation stage outputs
â”‚   â”œâ”€â”€ baseline_*/      # No memory baseline
â”‚   â”‚   â””â”€â”€ evaluation_results.json  # Baseline accuracy
â”‚   â”œâ”€â”€ selection_*/     # Lesson retrieval
â”‚   â”œâ”€â”€ with_memory_*/   # With retrieved lessons
â”‚   â”‚   â””â”€â”€ evaluation_results.json  # With-memory accuracy
â”‚   â””â”€â”€ analysis/        # Aggregated analysis reports
â”‚       â”œâ”€â”€ cost_analysis_train.txt
â”‚       â”œâ”€â”€ cost_analysis_val.txt
â”‚       â””â”€â”€ summary.json  # Reflective usage (label-guided)
â””â”€â”€ debug/               # Single-problem testing
    â””â”€â”€ [same structure + analysis/]
```

---

## ğŸ› Debug Workflows

Both pipelines have debug versions that run on **1 training + 1 validation problem**:

### Purpose:
- Inspect prompts in detail
- Verify lesson quality
- Check retrieval relevance
- Compare baseline vs with-memory
- Tune formatting and phrasing

### Training Problem: `2020-I-1` (geometry)
### Validation Problem: `2024_I_10` (geometry, matching domain)

### Key Inspection Points:
1. âœ… Lesson quality and specificity
2. âœ… Retrieval relevance (are top 2 lessons actually helpful?)
3. âœ… Prompt formatting (self-assessment + lessons)
4. âœ… Model behavior (did it use or ignore lessons?)
5. âœ… Performance impact (did memory help or hurt?)

---

## ğŸ“Š Expected Results

### Baseline (No Memory):
- **Target**: ~73% pass@1 on validation

### With Memory (Top 2 + Self-Assessment):
- **Goal**: â‰¥73% pass@1 (don't hurt performance)
- **Stretch Goal**: >75% pass@1 (beat baseline)

### Success Criteria:
- âœ… No easy problems (1-8) go from correctâ†’wrong
- âœ… At least 2-3 hard problems (9-15) go from wrongâ†’correct
- âœ… Overall pass@1 â‰¥ baseline

---

## ğŸš€ Quick Start

### Run Full Pipeline:
```bash
cd arc_memo
source .venv/bin/activate
export OPENAI_API_KEY="sk-..."
export OPENROUTER_API_KEY="sk-or-..."

# Choose one:
# Follow RUN_AIME_SELF_REFLECTIVE.md
# OR
# Follow RUN_AIME_LABEL_GUIDED.md
```

### Run Debug Version:
```bash
# Choose one:
# Follow RUN_AIME_SELF_REFLECTIVE_DEBUG.md
# OR
# Follow RUN_AIME_LABEL_GUIDED_DEBUG.md
```

---

## ğŸ“ Recent Changes

### v2.0 (2025-11-20)
- âœ… Renamed pipelines: 2P5 â†’ Self-Reflective, Correct-Only â†’ Label-Guided
- âœ… Organized outputs into dedicated directories
- âœ… Added debug workflows for both pipelines
- âœ… Reduced from 5 to 2 lessons per problem
- âœ… Added self-assessment prompt
- âœ… Standardized naming conventions

### v1.0 (Previous)
- âœ… Initial 2P5 pipeline (uncertain reflections)
- âœ… Initial correct-only pipeline
- âœ… Baseline 5 lessons per problem

---

## ğŸ” Deprecated Files

The following files are superseded by the new structure:

- ~~`RUN_AIME_2P5.md`~~ â†’ Use `RUN_AIME_SELF_REFLECTIVE.md`
- ~~`RUN_AIME_CORRECT_ONLY.md`~~ â†’ Use `RUN_AIME_LABEL_GUIDED.md`
- ~~`RUN_AIME_2P5_DEBUG.md`~~ â†’ Use `RUN_AIME_SELF_REFLECTIVE_DEBUG.md`
- ~~`RUN_AIME_QUICK.md`~~ â†’ Functionality merged into debug versions

---

## ğŸ“š Documentation

- `docs/27_self_assessment_prompt_for_lessons.md` - Design doc for self-assessment feature
- `CHANGES_TOP2_LESSONS.md` - Summary of top-k reduction from 5 to 2
- `docs/26_aime_abstraction_domain_template_bug_fix.md` - Historical bug fixes
- `aime_analysis/README.md` - **Analysis tools documentation** (evaluation, costs, filtering)

---

## ğŸ¯ Next Steps

1. Run debug versions to verify prompt quality
2. Run full pipelines on 60 validation problems
3. Compare results: self-reflective vs label-guided vs baseline
4. Iterate on lesson formatting based on inspection
5. Consider k=1 or k=3 if k=2 isn't optimal

