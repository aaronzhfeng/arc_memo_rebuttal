
# Augmenting the AIME Pipeline with Reflective Memory Integration

## Background and Motivation  
The current AIME problem-solving pipeline (`arc_memo`) operates in phases: **Solve → Thought_Process → Abstraction → Solve_with_Retrieval**. In the existing setup, only correctly solved problems contribute to the memory (concept abstraction). This means any reasoning from failed attempts is discarded. However, recent frameworks like *Dynamic Cheatsheet (DC)* and *Agentic Context Engineering (ACE)* demonstrate that **models can learn even from failures** by reflecting on mistakes and updating a persistent memory with corrected strategies. For example, Dynamic Cheatsheet improved Claude 3.5's accuracy on AIME exams from 23% to 50% by retaining algebraic insights across questions. The key idea is to **curate an evolving “cheatsheet” of insights**: if a solution approach is correct or useful, it’s stored; if it’s flawed, the model refines or prunes it to avoid propagating errors. This is analogous to the *Reflector* component in ACE, which examines solution traces to identify what failed vs. succeeded and distills useful lessons. In essence, even a wrong attempt can teach a right lesson.  

**Goal:** We propose augmenting the pipeline with a *reflective thought process stage* for unsolved problems. By **showing the model its incorrect solution alongside the correct answer, and prompting it to reflect**, we can extract *valuable strategies or corrections* from the failure. These reflections will feed into the abstraction step, enriching the memory with lessons learned from mistakes. This approach aligns with test-time learning paradigms (e.g. *Reflexion*) where an agent critiques its own failed attempts and updates an episodic memory to improve subsequent performance. 

## Reflective Thought Process Stage for Unsolved Problems  
**When a problem is answered incorrectly (or left unsolved) in the initial solve stage, we introduce a Reflective Thought Process step.** In this step, the model is given: (a) the problem statement, (b) its original incorrect answer (and any brief reasoning if available), and (c) the **correct answer** from the answer key. The model is then explicitly instructed to **analyze the discrepancy** – i.e., *why was the original solution wrong, and how should one correctly solve the problem?* The output is a *reflective reasoning trace* that identifies the mistake and outlines the correct line of thought. This reflective trace effectively serves as a corrected solution process, which can be just as valuable for abstraction as a directly correct solution would be.

To guide the model, we design a **prompt template with a few-shot example style**. The prompt will explicitly compare the wrong attempt to the right answer and ask for a revised reasoning. For example, we might use two mini-examples of reflection before the actual query, to illustrate the expected behavior. Below is a **template** (in simplified form) for the reflective prompt, with placeholders in curly braces and an example:

```text
You are a math expert helping to analyze solutions. For each problem, you will be given:
- The Problem.
- An initial Solution Attempt (with the answer given by the solver).
- The Correct Answer.

Your job: Explain **why the attempt was wrong and how to correctly solve the problem**. Identify any mistakes in the attempt, then provide a step-by-step corrected solution that leads to the correct answer.

**Example 1:**
Problem: A jar has 5 red and 3 blue marbles. If 2 marbles are drawn without replacement, what is the probability both are red?
Initial Attempt and Answer: "We treat it as independent draws: $P(\text{red, then red}) = 5/8 \times 5/8 = 25/64$. So the answer is $\frac{25}{64}$."
Correct Answer: $\frac{5}{14}$.
**Reflection:** The attempt assumed the draws were independent with replacement, which was a mistake. After drawing one red, the total marbles and red count change. The correct calculation is: $P(\text{red then red}) = \frac{5}{8} \times \frac{4}{7} = \frac{20}{56} = \frac{5}{14}$. The error was not accounting for the decreased total and red count on the second draw.

**Example 2:**
Problem: How many ways can you choose 2 people out of 5 people to form a team?
Initial Attempt and Answer: "Order might matter, so compute 5P2 = 5 \times 4 = 20 ways."
Correct Answer: $10$.
**Reflection:** The attempt counted each pair twice because order doesn’t matter in a combination. The correct approach is to use combinations: $\binom{5}{2} = \frac{5 \times 4}{2!} = 10$. The mistake was treating it as an ordered permutation instead of an unordered selection.

**Now your turn:**
Problem: {problem_text}
Initial Attempt and Answer: "{model_attempt}" (Incorrect)
Correct Answer: {correct_answer}
**Reflection:** 
``` 

In the above template, the model is shown two scenarios of incorrect attempts with their correct answers and reflective analyses. These few-shot exemplars demonstrate how to explicitly call out the mistake and then articulate the correct solution path. The prompt then ends with the new problem and the model’s own wrong answer, and asks for a **Reflection**. We emphasize phrases like *“explain why the attempt was wrong and how to correctly solve”* to clearly set the expectation. The model’s output should be a coherent explanation that first **diagnoses the error** in `{model_attempt}` and then provides the corrected reasoning leading to `{correct_answer}`.

**Key points for the reflective prompt design:**  

- *Include the correct answer:* Since we have ground-truth, we supply it to ensure the reflection converges on the right solution. This is akin to giving the model “feedback” on its output so it can analyze the gap. Dynamic Cheatsheet’s curator had to guess correctness without labels, but here we shortcut that by providing the answer, allowing a focused analysis of the mistake.  
- *Compare attempt vs correct outcome:* The prompt explicitly presents *both* the wrong attempt and the correct result. This encourages the model to pinpoint **where the solution went astray** – e.g. a faulty assumption, a calculation error, a missed case. The few-shot examples illustrate this comparison (e.g. *“The attempt assumed independence; the correct solution accounts for without replacement”*, or *“The attempt used permutations instead of combinations”*).  
- *Couple error analysis with a revised solution:* The reflection should not stop at pointing out the error; it should also walk through the proper solution. This ensures the output contains a *complete, correct chain of thought* for solving the problem. In effect, the model is reconstructing how it **should have** reasoned. This corrected reasoning will serve as the basis for abstraction.  
- *Tone and format:* We frame the model as a helpful analyst (“You are a math expert…”). The reflection answer itself is expected to be a well-structured explanation, possibly in a step-by-step format. We separate the context (Problem/Attempt/Correct Answer) from the model’s reflection by a clear delimiter (e.g. starting the answer with "**Reflection:**"). This makes it easier for the subsequent abstraction stage to parse the relevant content.  

With this reflective stage, even an initially unsolved problem yields a *thought process entry* – now a reflective one – that can be fed into the memory. This approach draws direct inspiration from the **Reflexion** framework, where *“after each task attempt, the agent critiques its own performance in natural language, then stores that feedback to induce better decision-making in subsequent trials”*. By integrating a reflection step, we enable the pipeline to capture **“valuable and generalizable insights” even from failures, by distilling them into corrected heuristics for future use**.

## Adjustments to the Abstraction (Memory Curation) Stage  
Once we have the reflective thought processes for the previously incorrect solutions, the **abstraction stage** can proceed largely as before – but we may adjust the prompt and handling to account for the nature of these reflections. Recall that the abstraction step (using prompts like `aime_lesson_from_trace_strict`) takes problem statements, solutions, and reasoning traces as input to produce abstracted *“lessons”* or reusable concepts. We want to ensure that the **lessons extracted from reflective traces focus on the corrected strategies rather than the mistakes**. Here are the suggested modifications and considerations:

- **Prompt emphasis on general insights:** If not already present, add an instruction in the abstraction prompt to *focus on general problem-solving principles or strategies demonstrated*. For example, we might append a guideline like: *“Ignore specific numerical values or story details; abstract the underlying strategy or correction that would be useful for other problems.”* This helps steer the model to derive a concept from the reflection. In practice, the reflection text will contain both the mistake description and the correct approach; the abstraction model should derive a positive lesson (and possibly a caution) from that. For instance, from the combination example reflection, the lesson could be *“When order doesn’t matter, use combinations (divide out permutations’ overcounting)”*. From the probability example, a lesson might be *“Account for changing totals in sequential draws (without replacement)”*. We want the abstraction to capture such general rules.  

- **Filtering out problem-specific content:** The strict abstraction prompt already imposes constraints (the `aime_lesson_from_trace_strict` variant includes explicit requirements, e.g. avoid problem-specific proper nouns or one-off values). We should ensure one of those constraints covers ignoring the fact that a solution came from a mistake. **The lessons should be formulated as general prescriptions, not just statements like “Don’t do X like in that attempt.”** For example, instead of a lesson saying *“The solver’s mistake was assuming independence,”* it should say *“Be careful: if events are without replacement, adjust probabilities accordingly.”* We might tweak the prompt to encourage phrasing lessons as **positive directives or general cautions**. This could be done by adding to the abstraction prompt template: *“For any solution that involved a correction of an error, frame the lesson as a general principle (e.g., how to do it correctly, or what pitfall to avoid in the future).”*  

- **Incorporating reflective traces seamlessly:** From a data handling perspective, the abstraction step can treat the reflective thought processes similar to normal solution traces. Each problem now has a `thought_process` entry – some will be original correct solutions, others will be reflective corrections. We likely don’t need a separate abstraction mechanism; we just feed all thought processes into the concept abstraction model. However, it may be wise to **tag or annotate** reflections internally (for debugging/analysis), even if the abstraction prompt doesn’t explicitly distinguish them. For instance, we could add a comment in the JSON or prompt context like “[This solution was generated via reflective analysis of an initial error]”. The abstraction model (GPT-4.1 in our pipeline) might not need this tag to do its job, but for our own analysis it could help to see which lessons came from reflections versus direct solves.  

- **Example abstraction prompt snippet:** Suppose the abstraction prompt currently does something like: *“You will be given a math problem, the solution, and the solver’s thought process. Extract up to 3 general lessons… etc.”* We should verify it handles cases where the thought process contains an error analysis. To be safe, our prompt snippet could be adjusted as:  

  > *“Below is a solver’s reasoning for a problem, which may include corrections of a previous mistake. Abstract the key mathematical insights or strategies that would be useful for solving similar problems. Focus on the **correct** approach described (ignore any discarded wrong paths except to note what to avoid). Provide the lessons as concise bullet points or sentences.”*  

  This ensures that if the reasoning says “the mistake was X, the correct method is Y,” the lesson will center on “Do Y (and beware of X).” We keep the lessons succinct and transferable, in line with DC’s philosophy of storing *“succinct, high-impact heuristics”*.  

- **Strictness and consistency:** Continue to enforce the strict format (e.g., one concept per lesson, no reference to the specific problem context or names). The reflective content might reference the specific problem’s details when explaining the error, but the abstraction should strip those away. If needed, we can instruct the model to *generalize any specific context*. For example, if a reflection mentions “marbles in a jar,” the lesson can refer generally to “drawing without replacement.” The existing strict prompt likely already encourages this kind of generalization, so minimal changes may be needed beyond highlighting the presence of corrected solutions.

By making these adjustments, we leverage the reflections fully: the abstraction module will **treat a corrected solution from a failed attempt just like a successful solve – extracting the core insight behind solving that problem.** This aligns with the memory curation in Dynamic Cheatsheet, where the curator *“evaluates the model’s output and keeps only the most useful, generalizable strategies”*, even refining faulty ones into correct form. In our case, the reflection step has already done the refinement; the abstraction step just needs to record the result as a lesson.

## Post-Reflection Verification and Memory Curation Strategies  
Incorporating reflections will increase the number of lessons in memory. It’s important to maintain **quality control** on these new entries so that the memory remains helpful and not noisy. We propose a couple of optional strategies after generating reflection-based abstractions:

- **Verification by re-solving (optional):** After obtaining a reflective thought process for a problem, we can test its utility by attempting to solve the *same* problem again using the newly generated knowledge. In practice, this could mean taking the reflection (or the abstracted lesson from it) and appending it to the context for the original problem, then seeing if the model now produces the correct answer. Since we are providing the correct answer in the reflection prompt, this is mostly a sanity check – but it can validate that the reflection indeed contains enough information to solve the problem. For instance, if the reflection analysis is sound, the model (with that reflection as a hint) should now confidently arrive at the correct answer. If it somehow still fails, that might indicate the reflection was incomplete or unclear. This step is analogous to an internal consistency check. It’s not strictly necessary for the pipeline (since we already know the correct answer), but it can serve as a **scoring mechanism for the reflection’s quality**. We could log a score (1 if the problem becomes solvable with the reflection, 0 if not) and use that to prioritize which lessons to keep. In general, we expect most reflections to pass this test, given the correct answer was known – but this could catch any cases where the model’s reflective explanation was flawed or off-target despite having the answer.  

- **Filtering trivial or overly specific lessons:** Not all mistakes are worth memorializing. For example, a reflection that essentially says “I made an arithmetic error” might lead to a lesson like *“Be careful with arithmetic calculations.”* While not wrong, this advice is very general and likely already implicit for the model (and appears in many contexts). Filling the memory with too many trivial reminders could dilute more substantive strategies. Therefore, we might implement a filtering step on the lessons generated. Possible approaches:
  - **Heuristic filtering:** Define simple rules to skip lessons that are too generic or problem-specific. E.g., if a lesson is just “Do basic arithmetic carefully” or “Read the question properly,” it might be filtered out as low value. Likewise, if a lesson explicitly includes a specific number/proper noun from a single problem, it might be too narrow (unless that number represents a known constant or formula). The strict prompt likely avoids the latter, but we should be vigilant.
  - **Clustering or redundancy check:** If many reflections yield the *same fundamental lesson*, we don’t need duplicates. For instance, if multiple problems had the mistake of ignoring order vs combinations, we might get similar lessons like “Use combinations for unordered choices” more than once. We can consolidate these. A simple method is to compare embeddings of the lesson texts and merge or drop near-duplicates, keeping one representative. Another method is to have a predefined set of known trivial lessons to exclude. The Dynamic Cheatsheet curator analogously strives to keep the memory **“succinct and high-impact” by pruning redundant or low-value content**.
  - **Model-based scoring:** We could ask the abstraction model (or another judgment model) to rate each lesson on general usefulness. For example, a prompt could present a lesson and ask “Is this a generally useful strategy or just a one-time note?” and only keep those marked useful. This is probably overkill for our scope, but it’s a conceivable extension for automatic curation.

- **Memory entry formatting:** We should ensure the memory entries resulting from reflections are formatted consistently with those from direct solves. Likely, each lesson will be stored with some key referencing the problem or concept. We might not need any flag that it came from a reflection in the runtime memory used for retrieval – the model just sees a pool of lessons. However, internally we might track source metadata for analysis. The crucial part is that the **content** of the memory is reliable and general. Thanks to the reflection, each entry, even if born from a failure, is actually a *validated*, corrected insight. This addresses a concern noted in DC: if a model is too weak and generates mostly flawed strategies, a naive memory could accumulate bad heuristics. Our approach avoids storing faulty heuristics because we only store the post-reflection corrected version of any initially faulty reasoning. In essence, we turn failed attempts into *learning experiences*, which is exactly the goal of using failed samples effectively at test time.

- **Continuous refinement:** As the memory grows, we might occasionally re-run the abstraction on the entire memory to condense or clean it (though in our pipeline, the memory is stored as a set of lessons and retrieved by similarity for new problems). If a later reflection provides a better version of a concept already in memory, the curator/abstraction step can update that concept. This is analogous to the DC curator performing (ii) **“refinement or removal of existing memory entries if an entry was incorrect or superseded by a more efficient strategy”**. For example, if initially we stored a somewhat narrow lesson and a later reflection produces a more comprehensive version, we should replace or augment the memory accordingly. Implementing this could be as simple as checking during abstraction if a new lesson subsumes an old one. In practice, since our memory entries are likely keyed by problem or concept IDs, we might allow multiple entries on similar topics, but the retrieval mechanism (similarity search) will surface the most relevant ones. Still, periodically reviewing memory for overlaps or conflicts is a good practice.  

In summary, after adding reflections, **the memory curation becomes even more crucial**: we are now curating both successes and failures (turned into successes). By verifying critical cases and pruning low-value lessons, we ensure the memory remains a high-quality knowledge base. This approach echoes lessons from the *Reflexion* and *ACE* literature: successful test-time learning requires disciplined curation of feedback. For instance, Reflexion agents improved by *“maintaining their own reflective text in an episodic memory buffer”* and using it to avoid repeating mistakes. ACE similarly emphasizes *“incremental updates that add refined strategies and remove low-value items, so knowledge grows without noise or drift”*. Our filtering and verification steps aim to achieve that: keep the memory **clean, correct, and focused**.

## Conclusion and Integration into `arc_memo`  
Adopting this reflective augmentation will change the `arc_memo` pipeline as follows:  

1. **Solve phase:** (Unchanged) The model attempts each problem. For each problem, we record the model’s final answer.  
2. **Thought Process phase:** (Modified) Instead of skipping unsolved problems, we invoke two different prompting strategies: for correctly solved problems, use the original prompt to generate an explanatory thought process as before; for incorrect or blank solutions, use the **reflective prompt template** to generate a post-mortem reasoning given the correct answer. This may be implemented as a conditional in the `aime_thought_process` script: check the solution against the answer key, and choose the prompt accordingly. The output is a JSON of thought processes for all problems – now every problem has a reasoning trace (some will be labeled as reflective analyses).  
3. **Abstraction phase:** (Slightly Modified) Use the thought processes from step 2 to prompt GPT-4.1 (`analysis_concepts` module) to extract lessons. We adjust the abstraction prompt with the guidelines above to accommodate reflective content. The function (`aime_lesson_from_trace_strict`) might be updated to include those additional instructions about focusing on corrected reasoning and general principles. The rest of this stage (generating `lessons.json`) remains the same. We should expect more lessons than before (since previously unsolved problems now contribute).  
4. **Solve with Retrieval phase:** (Unchanged in mechanism) When solving new problems (e.g. the validation or test set), the model will retrieve relevant lessons from memory and include them in the prompt. This step should automatically benefit from the richer memory. For example, if a new problem is similar to one that was initially failed but then reflected upon, the memory likely contains the strategy that was missing originally, increasing the chance the model now solves it correctly. This is precisely the scenario DC aims for – *“the model retains a toolkit of formulas and general patterns, so it doesn’t repeat the same mistakes”*.  

Overall, these changes integrate seamlessly with `arc_memo`’s design. They **mimic an expert learner: even when the model “gets it wrong,” it takes the time to figure out why and remember the fix.** By doing so, we expect the model’s performance to improve over time, as it accumulates corrected strategies in memory. This approach is grounded in emerging research on test-time learning and self-improvement for LLMs. Dynamic Cheatsheet has shown that allowing an LM to *“store and reuse accumulated strategies… without modifying model weights”* can yield substantial gains. ACE extends this by explicitly separating the reflection step, ensuring that *“reasoning paths that consistently produce strong results”* are identified and retained, while weak strategies are filtered out. Our proposal brings these ideas into the AIME solving context. By incorporating a reflective thought process and curated memory updates, `arc_memo`’s pipeline becomes a concrete implementation of *“learning from mistakes”* during inference. We transform the one-shot solution process into an iterative learning loop: **Solve → Reflect → Abstract → (reuse memory) → Solve Better**. This should lead to a more robust problem-solving system that continuously adapts and improves, even in the absence of traditional training. 

**References:** Dynamic Cheatsheet’s memory framework; ACE’s Generator-Reflector-Curator paradigm; Reflexion’s self-feedback mechanism, all of which underpin the strategies used here.
